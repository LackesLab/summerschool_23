{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1: Lade die Daten\n",
    "In der heutigen Übung wollen wir Unstrukturierte Daten mittel natürlicher Sprache durchsuchen.\n",
    "unter `tsa05` findest du eine Textdatei, die den Wikipedia Artikel über Deutschland enthält.\n",
    "\n",
    "Alternativ kannst du mit dem nachfolgenden Snippet auch einen anderen Artikel crawlen. Denke aber daran,\n",
    "mögliche Datenprobleme zu beheben, wenn sie in vermehrter / störender Form auftreten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_wikipedia_text(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get data: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    raw_text = \"\"\n",
    "    for p in paragraphs:\n",
    "        raw_text += p.get_text()\n",
    "    \n",
    "    return raw_text\n",
    "\n",
    "url = \"https://de.wikipedia.org/wiki/Deutschland\"\n",
    "text = fetch_wikipedia_text(url)\n",
    "print(text[:500])  # Print first 500 characters to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from txt\n",
    "\n",
    "# with open(...) as f:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2: Erstelle einzelne Datenchunks\n",
    "Generiere aus dem zusammenhängenden Text, Daten Chunks. Wähle dabei eine sinnvolle Split-Methode. Beachte, dass auch das Aleph Modell eine maximale Kontextlänge hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3: Embedding Generierung\n",
    "Doc LangChain: https://python.langchain.com/docs/integrations/text_embedding/aleph_alpha \n",
    "Doc AlephAlpha: https://docs.aleph-alpha.com/docs/introduction/luminous/\n",
    "Erstelle mithilfe von langchain und dem Aleph Alpha Modul Embeddings. Informiere dich über symetrische und asymetrische Embeddings.\n",
    "\n",
    "\n",
    "In der realen Anwendung würde die generierten Embeddings in einer Vektordatenbank gespeichert werden. Wenn du fit mit Docker bist und \n",
    "fit in der Programmierung bist, setze eine typesense Datenbank auf und speichere die Daten dort. https://typesense.org/\n",
    "Langchain bietet für typesense auch eine Integration: https://python.langchain.com/docs/integrations/vectorstores/typesense\n",
    "\n",
    "\n",
    "Ansonsten: Speichere die generierten Embeddings so ab, dass du sie einfach mithilfe der Cosinus Ähnlichkeit abfragen kannst.\n",
    "Erstelle dabei eine Struktur, die es dir auch ermöglicht den dazugehörigen Text zu speichern. \n",
    "Tipp: Zwei numpy arrays bzw eine numpy Array und eine Liste reichen theoretisch aus. Die beiden Objekte teilen sich dann die selben Indizes np.array[0] gehört zu list[0] usw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 4: Embedding Matching\n",
    "\n",
    "Implementiere eine Methode, mit der du die Ähnlichkeit zwischen dem Query Embedding und den restlichen Daten berechnen kannst. Füge einen top K Parameter hinzu.\n",
    "Das Ergebnis soll eine Liste mit Top K Textueller Ergebnisse sein, die wieder in ein LLM gesteckt werden können um eine Ausgabe in natürlicher Sprache zu generieren.\n",
    "Achte auf die Unterschiede zwischen Cosinus Distanz und Cosinus Ähnlichkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 5: Ausgabe in natürlich Sprache\n",
    "Verwende Langchain um mit der Aleph Alpha API zu kommunuzieren und eine Ausgabe in natürlicher Sprache darzustellen. \n",
    "Überlege dir eine Frage, die du aus dem Wikipedia Artikel beantwortet haben möchtest und übermittle sie via API\n",
    "- Evtl hilft dir das Modul LLMChain\n",
    "- Weitere Infos https://python.langchain.com/docs/integrations/llms/aleph_alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: All-in-One\n",
    "Packe die gesamte Funktionaltiät des Querying in eine Funktion. Die Funktion soll die Frage als Parameter erhalten und als return die Antwort in natürlicher Sprache liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
